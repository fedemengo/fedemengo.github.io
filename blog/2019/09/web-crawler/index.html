<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Designing an efficient webcrawler | fedemengo</title>
    <meta name="author" content="fedemengo  ">
    <meta name="description" content="What's powering every search engine?">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/code.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fedemengo.github.io/blog/2019/09/web-crawler/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>

    <!-- Online 3D Viewer -->
    <script src="/assets/js/o3dv/o3dv.min.js"></script>
    <script>
        OV.SetExternalLibLocation ('libs'); // tell the engine where to find the libs folder
        OV.Init3DViewerElements (); // init all viewers on the page
    </script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">fedemengo</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Designing an efficient webcrawler</h1>
    <p class="post-meta">September 17, 2019</p>
    <p class="post-tags">
      <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>
        ·  
        <a href="/blog/category/system-design">
          <i class="fas fa-tag fa-sm"></i> system-design</a>  
          

    </p>
  </header>

  <article class="post-content">
    <h2 id="terminology">Terminology</h2>

<ul>
  <li>Web crawlers generally start from a <strong>seed</strong> web page and can collect data form many more pages by following all outgoing links <!--more-->
</li>
  <li>Web crawlers can be <strong>exhaustive</strong> or <strong>topical/focused</strong> depending on the pages they will inspect (follow one topic, follow pages newer than a given data and so on..)</li>
  <li>A crawling strategy can be selective on the maximum number of pages to fetch: <strong>short</strong> vs <strong>long crawls</strong>
</li>
</ul>

<h2 id="architecture">Architecture</h2>

<h3 id="components">Components</h3>

<ul>
  <li>Frontier</li>
  <li>Fetcher</li>
  <li>Parser
    <ul>
      <li>url extractor</li>
      <li>url filtering</li>
      <li>url prioritizer</li>
    </ul>
  </li>
  <li>Storage</li>
</ul>

<p>In addition to those component, that consists of the bare minium for a simple crawler,  a distributed web crawler will require  a url distributor that will take care of assigning url to each crawler (right after extracting them) based on on the respective locality.</p>

<h4 id="frontier">Frontier</h4>

<p>A <strong>frontier/open-list</strong> (list of unvisited urls) is initialized with a seed and store all the unvisited urls. Can be an in-memory data structure for small crawlers while it usually stores the urls on disk for large scale systems. It’s necessary to avoid adding duplicates to the frontier, so a separate hash-table or Bloom filter can be used to avoid the problem. When the frontier reaches it’s maximum capacity, only <strong>one</strong> new url can be added from the current page. Urls extracted from a page are generally assigned a score depending on their importance according to the crawling strategy.</p>

<p>It may happens that the frontier contains many urls that point to the same or similar page, this problem is referred to as the <strong>spider-trap</strong>. In this case it’s reasonable to only accept \(k\) urls for the same domain every \(n\) urls processed.crawcraw</p>

<p>When urls are assigned a crawling priority it useful to implement the frontier as priority queue. The problem when using a disk-based priority queue is that is necessary to rearrange elements periodically and that would results in many disk seeks, consequentially limiting the number of insertion per second.</p>

<p>A possible solution is to discretize the priority and have as many frontier as interval of priority.</p>

<h4 id="fetching">Fetching</h4>

<p>An http client is necessary to fetch a webpage. It needs to be configured with a timeout (to avoid wasting waiting for a response too long), it has to to inspect the header of page (for redirection, last modified date and so on). Before fetching a page from a new host, the crawler should check for a <code class="language-plaintext highlighter-rouge">robots.txt</code> file that inform the crawler to skip specific urls.</p>

<p>In a distributed crawler it’s important to avoid issuing multiple overlapping request to the same server (denial-of-service), to do this one solution could be to map a domain to a single crawling unit. Another way to avoid sending too many requests consists on adding a delay before requesting another page form the same domain (for example 10 times the time it took to download the last page); in real implementation there is generally just one frontier per worker and many backend frontiers (in the url distributor), each one assigned to a specific domain.</p>

<p>Other data-structure used to improved the performances of a crawler are the robot.txt cache and the DNS cache.</p>

<h4 id="parsing">Parsing</h4>

<p>A <strong>crawling loop</strong> fetch the next url in the frontier, extract application specific data and add the page’urls to the frontier.</p>

<p>Before adding new urls to a page, such url need to be <strong>canonicalized</strong> meaning it’s necessary to transform the url applying certain criteria, the key is applying them consistently</p>

<ul>
  <li>Convert protocol and hostname to lowercase</li>
  <li>Remove anchor or references</li>
  <li>Perform url-encoding of special characters</li>
  <li>Add trailing <code class="language-plaintext highlighter-rouge">/</code> when necessary (<code class="language-plaintext highlighter-rouge">x.y</code> and <code class="language-plaintext highlighter-rouge">x.y/</code>)</li>
  <li>Remove default web pages (<code class="language-plaintext highlighter-rouge">x.y/</code> and <code class="language-plaintext highlighter-rouge">x.y/index.html</code>)</li>
  <li>Resolve local path</li>
  <li>Leave port number unless is port <code class="language-plaintext highlighter-rouge">80</code> (default)</li>
  <li>Known mirrors</li>
  <li>Consider limiting the url size to 128/256 characters</li>
</ul>

<p>When extracting data from a page, it’s a good practice to <strong>stoplist</strong> (remove common stop works) and <strong>stem</strong> (conflate words to a common root).</p>

<p>In the case of a distributed crawler that partitions the url space among each replica, it’s important to have a mean to send an extracted url to the appropriate instance: this can be achieved with p2p communication (consistent hashing/DHT or using a central source of urls distribution)</p>

<h2 id="algorithms">Algorithms</h2>

<h3 id="naive-best-first-crawler">Naive Best-First crawler</h3>

<p>Each fetched page is represented as list of words weighted by their frequency, it then computes the similarity between the page and the description provided by the user. A similarity function can be</p>

\[sim(q, p) = \dfrac{Vq \cdot{ Vp}}{\mid\mid Vq\mid\mid \cdot \mid\mid Vq \mid\mid}\]

<p>Where \(Vq, Vp\) are the term frequency vector for query and fetched page and \(\mid\mid v \mid\mid\) is the Euclidean norm of the vector \(v\)</p>

<h3 id="sharksearch">SharkSearch</h3>

<p>This algorithm uses the anchor-text, anchor context and inherited scores to assigned a more refined score by also keeping track of the value of the pages on a path (if such pages are not important it stops crawling down the path, a depth bound is also used as upper bound). The following function can be used</p>

\[score(url) = \gamma \cdot inherited(url) + (1-\gamma) \cdot neighborhood(url)\]

<p>where \(\gamma &lt; 1\), \(inherited\) is obtain from the ancestor of the page and \(neighborhood\) is calculated using anchor-text and anchor context.</p>

<p>The \(inherited\) score is computed as</p>

\[inherited(url) =
\begin{cases}
    \delta \cdot sim(q, p) &amp; \mbox{if } sim(q, p) &gt; 0 \\
    \delta \cdot inherited(p) &amp; \mbox{otherwise}
\end{cases}\]

<p>where \(\delta &lt; 1\), \(q\) is the query and \(p\) is the page from which the url is extracted.</p>

<p>while the \(neighborhood\) is calculate as</p>

\[neighborhood(url) = \beta \cdot anchor(url) + (1-\beta) \cdot context(url)\]

<p>where \(\beta &lt; 1\), \(anchor(url) = sim(q, anchorText)\) and</p>

\[context(url) =
\begin{cases}
    1 &amp; \mbox{if } anchor(url) &gt; 0 \\
    sim(q, augContext) &amp; \mbox{otherwise}
\end{cases}\]

<p>The algorithms is defined with as a parametrized function \(SharkSearch(d, \gamma, \delta, \beta)\)</p>

<h3 id="advanced">Advanced</h3>

<p>Other advanced crawler are <strong>focused crawlers, context focused crawler</strong> and <strong>InfoSpiders</strong></p>

<h2 id="page-importance">Page importance</h2>

<ul>
  <li>Keyword in document: depends on the number and frequency of keywords in the query that the page contains</li>
  <li>Similarity to a query: generally used when the query is a relatively long text</li>
  <li>Similarity to seed page: calculated using the similarity function between all seed pages combined and the crawled page</li>
  <li>Classifier score: either a boolean or continuos relevance score assigned to each page using a trained classifier</li>
  <li>Retrieval system rank: \(N\) different crawlers (namely using different strategy) are started form the same seeds and allowed to crawl $P\(pages, once the\)N \cdot P$$ have been crawled, they get ranked against the initial query using some retrieval system.</li>
  <li>Link bases popularity: PageRank, HITS or simpler version such as using the number of in-links to the crawled page</li>
</ul>

<h3 id="gotchastips">Gotchas/Tips</h3>

<ul>
  <li>Consistent hashing to partition the urls</li>
  <li>Keep seen-urls in a disk-based hash table that store them sparsely and use, for example, the first \(k\) bit of the hash to identify the disck block.</li>
</ul>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://dollar.biz.uiowa.edu/~gpant/Papers/crawling.pdf" rel="external nofollow noopener" target="_blank">Crawling the Web</a></li>
  <li><a href="http://infolab.stanford.edu/~olston/publications/crawling_survey.pdf" rel="external nofollow noopener" target="_blank">Web Crawling</a></li>
</ul>

  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/04/nvim-evil-regex/">How I made nvim 300x faster</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/10/visualizing-cellular-automata/">Visualizing cellular automata</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/10/poincare-recurrence-time/">Poincaré recurrence time in cellular automata</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/01/tomtom-spark-linux/">TomTom Spark on Linux</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/11/ssh-local-forwarding/">SSH Local Forwarding</a>
  </li>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 fedemengo  . Last updated: April 28, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
